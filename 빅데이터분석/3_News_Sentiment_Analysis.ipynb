{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fBiBoBZSm0Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-MPjPFJmgJk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "drive.mount(\"/content/gdrive/\") # ì§€ì • ìœ„ì¹˜ì™€ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "qAn7sGvE7jsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/gdrive/MyDrive/GMSW/project/result/'\n",
        "df = pd.read_csv(file_path+'news_topic_df_ssk.csv')"
      ],
      "metadata": {
        "id": "lw47CACwmsMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ê°ì„±ë¶„ì„"
      ],
      "metadata": {
        "id": "vU0AWBK5erbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê°ì„± ë¶„ë¥˜"
      ],
      "metadata": {
        "id": "nmxk0X5hm0SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU ì„¤ì • (Colab ì‚¬ìš© ì‹œ í•„ìˆ˜! ì†ë„ 10ë°° ì°¨ì´)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"ì‚¬ìš© ì¥ì¹˜: {'GPU (ë¹ ë¦„)' if device == 0 else 'CPU (ëŠë¦¼, ì¸ë‚´ì‹¬ í•„ìš”)'}\")"
      ],
      "metadata": {
        "id": "YR2vOIqkg3-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸ˆìœµ íŠ¹í™” ëª¨ë¸ ë¡œë“œ (snunlp/KR-FinBert-SC): ì„œìš¸ëŒ€ NLP ë©ì—ì„œ ë§Œë“  í•œêµ­ì–´ ê¸ˆìœµ ê°ì„±ë¶„ì„ ëª¨ë¸\n",
        "model_name = \"snunlp/KR-FinBert-SC\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# ê³ ì† íŒŒì´í”„ë¼ì¸ ìƒì„± (batch_size=32 ì¶”ì²œ)\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device, # GPU ì‚¬ìš©\n",
        "    batch_size=32, # í•œ ë²ˆì— 32ê°œì”© ì²˜ë¦¬ (ì†ë„ í•µì‹¬!)\n",
        "    truncation=True,\n",
        "    max_length=128 # ì œëª©ì´ë‹ˆê¹Œ 128 í† í°ì´ë©´ ì¶©ë¶„\n",
        ")\n",
        "\n",
        "# ë¶„ì„ ì‹¤í–‰\n",
        "titles = df['title'].tolist()\n",
        "results = []\n",
        "\n",
        "for output in tqdm(sentiment_analyzer(titles), total=len(titles)):\n",
        "    results.append(output)\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥\n",
        "# label(positive/negative/neutral)ê³¼ score(í™•ì‹ ë„) ë¶„ë¦¬\n",
        "df['Sentiment_Label'] = [res['label'] for res in results]\n",
        "df['Sentiment_Score'] = [res['score'] for res in results]\n",
        "\n",
        "# Labelì„ ìˆ«ìë¡œ ë³€í™˜: positive -> 1, neutral -> 0, negative -> -1\n",
        "label_map = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
        "df['Sentiment_Num'] = df['Sentiment_Label'].map(label_map)"
      ],
      "metadata": {
        "id": "mTAqDwhCdsHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # íŒŒì¼ ì €ì¥\n",
        "# df.to_csv(\"/content/gdrive/MyDrive/GMSW/project/result/news_topic_sentiment_result_ssk.csv\", index=False)"
      ],
      "metadata": {
        "id": "5ovajC2IdsCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë¶„ì„"
      ],
      "metadata": {
        "id": "82lg9Fu5m2j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/gdrive/MyDrive/GMSW/project/result/'\n",
        "df = pd.read_csv(file_path+'news_topic_sentiment_result_ssk.csv')"
      ],
      "metadata": {
        "id": "s_1JgDAzq5pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "fVsrWxTjmkbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "df_clean = df[df['topic'].isin(topics)].copy()\n",
        "df_clean = df_clean[df_clean['Sentiment_Score']>=0.5].copy()\n",
        "\n",
        "print(f\"ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ -> ì •ì œëœ ë°ì´í„°: {len(df_clean)}ê°œ\")"
      ],
      "metadata": {
        "id": "q9w5T1zFpmhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# 1. í† í”½ë³„ í‰ê·  ê°ì„± ì ìˆ˜ ê³„ì‚°\n",
        "# (Sentiment_Numì€ ê¸ì •=1, ì¤‘ë¦½=0, ë¶€ì •=-1ë¡œ ë³€í™˜ëœ ìˆ˜ì¹˜ë¼ê³  ê°€ì •)\n",
        "# ë§Œì•½ KR-FinBERT Score(0~1)ë¼ë©´ Sentiment_Labelì„ ì°¸ê³ í•˜ì—¬ -1~1ë¡œ ìŠ¤ì¼€ì¼ë§ í•„ìš”\n",
        "# Sentiment_Score(í™•ì‹ ë„)ê°€ ì•„ë‹ˆë¼ ê¸/ë¶€ì • ë¼ë²¨ì„ ìˆ˜ì¹˜í™”í•œ ê²ƒì„ ì“´ë‹¤ê³  ê°€ì •\n",
        "\n",
        "# Sentiment_Num: ê¸ì •(1), ì¤‘ë¦½(0), ë¶€ì •(-1)\n",
        "topic_sentiment = df_clean.groupby('Topic_Name')['Sentiment_Num'].mean().reset_index()\n",
        "topic_sentiment = topic_sentiment.sort_values('Sentiment_Num', ascending=True)\n",
        "\n",
        "# 2. ì‹œê°í™” (Bar Chart)\n",
        "fig = px.bar(topic_sentiment,\n",
        "             x='Sentiment_Num',\n",
        "             y='Topic_Name',\n",
        "             orientation='h',\n",
        "             title='<b>í† í”½ë³„ í‰ê·  ê°ì„± ì ìˆ˜ (Market Sentiment by Topic)</b>',\n",
        "             text_auto='.3f',\n",
        "             color='Sentiment_Num',\n",
        "             color_continuous_scale='RdBu') # ë¹¨ê°•(ë¶€ì •) ~ íŒŒë‘(ê¸ì •)\n",
        "\n",
        "fig.update_layout(xaxis_title=\"í‰ê·  ê°ì„± ì ìˆ˜ (-1: ë¶€ì •, +1: ê¸ì •)\", yaxis_title=\"\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FR9FngOym5_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "stock = pd.read_csv('/content/gdrive/MyDrive/GMSW/project/data/stock_price.csv')\n",
        "# stock.head()\n",
        "\n",
        "# Min-Max ì •ê·œí™”\n",
        "def min_max_scale(series):\n",
        "    return (series - series.min()) / (series.max() - series.min())\n",
        "\n",
        "# ì •ê·œí™”ëœ ì»¬ëŸ¼ ìƒì„±\n",
        "stock['Scaled_samsung_price'] = min_max_scale(stock['samsung_price'])\n",
        "stock['Scaled_skhynix_price'] = min_max_scale(stock['skhynix_price'])\n",
        "\n",
        "# 1. ì¼ë³„ í‰ê·  ê°ì„± ì ìˆ˜ ê³„ì‚°\n",
        "daily_sentiment = df_clean.groupby('date')['Sentiment_Num'].mean()\n",
        "# ì´ë™í‰ê· (MA)ì„ ì ìš©í•˜ë©´ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê³  ì¶”ì„¸ë¥¼ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤ (3ì¼ or 5ì¼)\n",
        "daily_sentiment_ma = daily_sentiment.rolling(window=5).mean()\n",
        "\n",
        "# 2. ê¸°ì¡´ ì´ì¤‘ì¶• ì½”ë“œ í™œìš©\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "# [ì™¼ìª½] ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ (ì´ë™í‰ê· )\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=daily_sentiment_ma.index, y=daily_sentiment_ma.values,\n",
        "               name=\"ë‰´ìŠ¤ ê°ì„± ì§€ìˆ˜ (5ì¼ ì´í‰)\",\n",
        "               line=dict(color='purple', width=1)\n",
        "              ),\n",
        "    secondary_y=False\n",
        ")\n",
        "\n",
        "# [ì˜¤ë¥¸ìª½] SKí•˜ì´ë‹‰ìŠ¤ ì£¼ê°€ (Scaled)\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=stock['date'], y=stock['Scaled_skhynix_price'],\n",
        "               name=\"SKí•˜ì´ë‹‰ìŠ¤ (Scaled)\", line=dict(color='royalblue', dash='dot')),\n",
        "    secondary_y=True\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=stock['date'], y=stock['Scaled_samsung_price'],\n",
        "               name=\"ì‚¼ì„± (Scaled)\", line=dict(color='red', dash='dot')),\n",
        "    secondary_y=True\n",
        ")\n",
        "\n",
        "fig.update_layout(title=\"<b>ë‰´ìŠ¤ ê°ì„± ë³€í™”ì™€ ì£¼ê°€ ì¶”ì„¸</b>\",\n",
        "                  legend=dict(orientation=\"h\", y=1.1, x=0, xanchor='left'), # ë²”ë¡€ ìœ„ë¡œ ì˜¬ë¦¬ê¸°\n",
        "                  )\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PybRAbZHm57U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# [ì‚¬ì „ ì„¤ì •] í† í”½ ë¶„ë¥˜ (ì‚¬ìš©ì ë°ì´í„°ì˜ í† í”½ ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤)\n",
        "# ---------------------------------------------------------\n",
        "# df_cleanì˜ í† í”½ ì´ë¦„ì´ ë“¤ì–´ìˆëŠ” ì»¬ëŸ¼ëª… (ì˜ˆ: 'Name', 'Topic_Name', 'Custom_Name' ë“± í™•ì¸ í•„ìš”)\n",
        "topic_col = 'Topic_Name'  # <-- ë³¸ì¸ ë°ì´í„° ì»¬ëŸ¼ëª…ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
        "\n",
        "# 1. ì‹¤ì /í€ë”ë©˜í„¸ ê´€ë ¨ í† í”½ ë¦¬ìŠ¤íŠ¸\n",
        "earnings_topics = ['ì‚°ì—…íŠ¸ë Œë“œ/ì‹¤ì /í€ë”ë©˜í„¸']\n",
        "# (ì‹œí™©/ë ë¦¬ë„ ìƒìŠ¹ ì¬ë£Œë¡œ ë³¸ë‹¤ë©´ í¬í•¨, ìˆœìˆ˜ ì‹¤ì ë§Œ ë³´ë ¤ë©´ 'ì‹œí™©/ë ë¦¬' ì œì™¸)\n",
        "\n",
        "# 2. ì •ì¹˜/ê±°ì‹œ/ì •ì±… ê´€ë ¨ í† í”½ ë¦¬ìŠ¤íŠ¸\n",
        "# politics_topics = ['ê¸€ë¡œë²Œ ê±°ì‹œ ì •ì¹˜', 'ì •ì±…']\n",
        "politics_topics = ['í˜‘ë ¥ ê¸°ëŒ€ê°']\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# [ë°ì´í„° ì „ì²˜ë¦¬]\n",
        "# ---------------------------------------------------------\n",
        "# ì£¼ê°€ ë°ì´í„° ë¡œë“œ ë° ìŠ¤ì¼€ì¼ë§ (ê¸°ì¡´ ì½”ë“œ ë™ì¼)\n",
        "stock = pd.read_csv('/content/gdrive/MyDrive/GMSW/project/data/stock_price.csv')\n",
        "\n",
        "def min_max_scale(series):\n",
        "    return (series - series.min()) / (series.max() - series.min())\n",
        "\n",
        "stock['Scaled_samsung_price'] = min_max_scale(stock['samsung_price'])\n",
        "stock['Scaled_skhynix_price'] = min_max_scale(stock['skhynix_price'])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# [ê°ì„± ì ìˆ˜ ë¶„ë¦¬ ê³„ì‚°]\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 1. ì‹¤ì (Earnings) ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ ë° 5ì¼ ì´í‰ ê³„ì‚°\n",
        "mask_earnings = df_clean[topic_col].isin(earnings_topics)\n",
        "daily_earnings = df_clean[mask_earnings].groupby('date')['Sentiment_Num'].mean()\n",
        "daily_earnings_ma = daily_earnings.rolling(window=5).mean()\n",
        "\n",
        "# 2. ì •ì¹˜(Politics/Macro) ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ ë° 5ì¼ ì´í‰ ê³„ì‚°\n",
        "mask_politics = df_clean[topic_col].isin(politics_topics)\n",
        "daily_politics = df_clean[mask_politics].groupby('date')['Sentiment_Num'].mean()\n",
        "daily_politics_ma = daily_politics.rolling(window=5).mean()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# [ì‹œê°í™”]\n",
        "# ---------------------------------------------------------\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "colors = ['#FFA07A', '#20B2AA', '#87CEFA', '#fbe383']\n",
        "\n",
        "# [ì™¼ìª½ ì¶• 1] ì‹¤ì /í€ë”ë©˜í„¸ ê°ì„± (ì§„í•œ ì´ˆë¡ìƒ‰ - ê¸ì •ì  ì´ë¯¸ì§€)\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=daily_earnings_ma.index, y=daily_earnings_ma.values,\n",
        "               name=\"ì‹¤ì /í€ë”ë©˜í„¸ ê°ì„± (5ì¼ ì´í‰)\",\n",
        "               line=dict(color='#87CEFA', width=1),\n",
        "               fill='tozeroy', opacity=0.3\n",
        "               ),\n",
        "    secondary_y=False\n",
        ")\n",
        "\n",
        "# [ì™¼ìª½ ì¶• 2] ì •ì¹˜/ê±°ì‹œ ê°ì„± (ì§„í•œ ë¹¨ê°„ìƒ‰ - ê²½ê³ /ë¶ˆí™•ì‹¤ì„± ì´ë¯¸ì§€)\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=daily_politics_ma.index, y=daily_politics_ma.values,\n",
        "               name=\"í˜‘ë ¥ ê¸°ëŒ€ê° (5ì¼ ì´í‰)\",\n",
        "               line=dict(color='#FFA07A', width=1),\n",
        "               fill='tozeroy', opacity=0.3\n",
        "               ),\n",
        "    secondary_y=False\n",
        ")\n",
        "\n",
        "# [ì˜¤ë¥¸ìª½ ì¶•] ì£¼ê°€ ë°ì´í„° (ì ì„ ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê°ì„± ì§€ìˆ˜ì™€ êµ¬ë¶„)\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=stock['date'], y=stock['Scaled_skhynix_price'],\n",
        "               name=\"SKí•˜ì´ë‹‰ìŠ¤ (Scaled)\",\n",
        "               line=dict(color='royalblue', dash='dot', width=1.5)),\n",
        "    secondary_y=True\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=stock['date'], y=stock['Scaled_samsung_price'],\n",
        "               name=\"ì‚¼ì„±ì „ì (Scaled)\",\n",
        "               line=dict(color='red', dash='dot', width=1.5)),\n",
        "    secondary_y=True\n",
        ")\n",
        "\n",
        "# ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
        "fig.update_layout(\n",
        "    title=\"<b>ë‰´ìŠ¤ í† í”½ë³„(ì‹¤ì  vs í˜‘ë ¥ ê¸°ëŒ€ê°) ê°ì„± ë³€í™”ì™€ ì£¼ê°€ ì¶”ì„¸ ë¹„êµ</b>\",\n",
        "    template='plotly_white',\n",
        "    legend=dict(orientation=\"h\", y=1.1, x=0.5, xanchor='center'),\n",
        "    xaxis_title=\"ë‚ ì§œ\",\n",
        "    yaxis_title=\"ê°ì„± ì ìˆ˜ (ì‹¤ì„ )\",\n",
        "    yaxis2_title=\"ì£¼ê°€ (ì ì„ , Scaled)\",\n",
        "    hovermode=\"x unified\" # ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œ ëª¨ë“  ë°ì´í„° í•¨ê»˜ ë³´ê¸°\n",
        ")\n",
        "\n",
        "# yì¶• ë²”ìœ„ ê³ ì • (ê°ì„± ì ìˆ˜ ë¹„êµ ìš©ì´í•˜ê²Œ)\n",
        "fig.update_yaxes(range=[-1, 1], secondary_y=False) # ê°ì„± ì ìˆ˜ëŠ” -1 ~ 1 ì‚¬ì´\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6hNAEJZhRMXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZb4IOZZRMQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê²€ì¦"
      ],
      "metadata": {
        "id": "069Xqc1OGSOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "E6OcDXtqGWiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë²¤íŠ¸ ì‹œì‘ì¼ ì§€ì •\n",
        "event_start_date = \"2025-10-16\"\n",
        "\n",
        "# ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
        "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
        "threshold_date = pd.to_datetime(event_start_date)\n",
        "\n",
        "# Pre: ì´ë²¤íŠ¸ ì´ì „ / Post: ì´ë²¤íŠ¸ ì´í›„ (ê¸‰ë“± êµ¬ê°„)\n",
        "group_pre = df_clean[df_clean['date'] < threshold_date]\n",
        "group_post = df_clean[df_clean['date'] >= threshold_date]\n",
        "\n",
        "print(f\"ë¶„ì„ ê¸°ê°„ ì„¤ì •: {event_start_date} ê¸°ì¤€\")\n",
        "print(f\"Pre-Event í‘œë³¸ ìˆ˜: {len(group_pre)}ê°œ\")\n",
        "print(f\"Post-Event í‘œë³¸ ìˆ˜: {len(group_post)}ê°œ\")"
      ],
      "metadata": {
        "id": "myqpgTx3Gj4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T-test 1: ê°ì„± ì ìˆ˜ (Sentiment)\n",
        "# \"ê¸‰ë“± í›„ì— ë‰´ìŠ¤ê°€ ë” ê¸ì •ì ìœ¼ë¡œ ë³€í–ˆëŠ”ê°€?\"\n",
        "t_stat_sent, p_val_sent = stats.ttest_ind(\n",
        "    group_post['Sentiment_Num'],\n",
        "    group_pre['Sentiment_Num'],\n",
        "    equal_var=False # ë“±ë¶„ì‚° ê°€ì • X (Welch's t-test) -> ë” ì•ˆì „í•œ ë°©ë²•\n",
        ")\n",
        "\n",
        "# T-test ìˆ˜í–‰ 2: ê´€ì‹¬ë„/ë³´ë„ëŸ‰ (Volume)\n",
        "# \"ê¸‰ë“± í›„ì— í•˜ë£¨ì— ìŸì•„ì§€ëŠ” ê¸°ì‚¬ëŸ‰ì´ ëŠ˜ì—ˆëŠ”ê°€?\"\n",
        "# ì¼ë³„ ê¸°ì‚¬ ìˆ˜ë¡œ ì§‘ê³„í•´ì•¼ í•¨\n",
        "vol_pre = group_pre.groupby('date').size()\n",
        "vol_post = group_post.groupby('date').size()\n",
        "\n",
        "t_stat_vol, p_val_vol = stats.ttest_ind(\n",
        "    vol_post,\n",
        "    vol_pre,\n",
        "    equal_var=False\n",
        ")"
      ],
      "metadata": {
        "id": "HGn6x4okm53o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ“¢ [ìµœì¢… ê°€ì„¤ ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸]\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"\\n1. ê°ì„± ì ìˆ˜ ì°¨ì´ (H1 ê²€ì¦)\")\n",
        "print(f\"   - Pre í‰ê· : {group_pre['Sentiment_Num'].mean():.4f}\")\n",
        "print(f\"   - Post í‰ê· : {group_post['Sentiment_Num'].mean():.4f}\")\n",
        "print(f\"   - T-statistic: {t_stat_sent:.4f}\")\n",
        "print(f\"   - P-value: {p_val_sent:.4e} ({'â­â­ ìœ ì˜í•¨' if p_val_sent < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})\")\n",
        "\n",
        "print(f\"\\n2. ê´€ì‹¬ë„(ë³´ë„ëŸ‰) ì°¨ì´ (H2 ê¸°ì´ˆ)\")\n",
        "print(f\"   - Pre ì¼í‰ê·  ë³´ë„ëŸ‰: {vol_pre.mean():.1f}ê±´\")\n",
        "print(f\"   - Post ì¼í‰ê·  ë³´ë„ëŸ‰: {vol_post.mean():.1f}ê±´\")\n",
        "print(f\"   - T-statistic: {t_stat_vol:.4f}\")\n",
        "print(f\"   - P-value: {p_val_vol:.4e} ({'â­â­ ìœ ì˜í•¨' if p_val_vol < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "ukN8pYP7dr_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‹œê°í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ê·¸ë¨ (Histogram)\n",
        "# barmode='overlay': ë‘ ê·¸ë˜í”„ë¥¼ ê²¹ì³ì„œ ë¹„êµ\n",
        "# histnorm='probability density': ë°ì´í„° ê°œìˆ˜ê°€ ë‹¬ë¼ë„ ë¹„ìœ¨ë¡œ ë¹„êµ ê°€ëŠ¥í•˜ê²Œ í•¨\n",
        "\n",
        "df_clean['Period'] = df_clean['date'].apply(\n",
        "    lambda x: 'Post-Event' if x >= threshold_date else 'Pre-Event'\n",
        ")\n",
        "\n",
        "fig = px.histogram(df_clean,\n",
        "                   x=\"Sentiment_Num\",\n",
        "                   color=\"Period\",\n",
        "                   barmode=\"overlay\",\n",
        "                   nbins=20, # ë§‰ëŒ€ ê°œìˆ˜ ì¡°ì ˆ\n",
        "                   histnorm='probability density', # í•µì‹¬: ê°œìˆ˜ê°€ ì•„ë‹Œ 'ë°€ë„'ë¡œ ë¹„êµ\n",
        "                   title=\"<b>ì´ë²¤íŠ¸ ì „í›„ ê°ì„± ì ìˆ˜ ë°€ë„ ë¹„êµ (Histogram)</b>\",\n",
        "                   color_discrete_map={'Pre-Event': 'gray', 'Post-Event': 'red'},\n",
        "                   opacity=0.6, # íˆ¬ëª…ë„ ì¡°ì ˆ\n",
        "                  )\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"ê°ì„± ì ìˆ˜ (-1: ë¶€ì •, 0: ì¤‘ë¦½, +1: ê¸ì •)\",\n",
        "    yaxis_title=\"ë¹„ì¤‘ (Density)\",\n",
        "    template=\"plotly_white\",\n",
        "    legend=dict(orientation=\"h\", y=1.1, x=0, xanchor='left'), # ë²”ë¡€ ìœ„ë¡œ ì˜¬ë¦¬ê¸°\n",
        ")\n",
        "\n",
        "# í‰ê· ì„  ì¶”ê°€ (ì´ê²Œ í•µì‹¬!)\n",
        "# Pre í‰ê· ì„  (íšŒìƒ‰ ì ì„ )\n",
        "fig.add_vline(x=0.2697, line_width=3, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Pre\")\n",
        "# Post í‰ê· ì„  (ë¹¨ê°• ì‹¤ì„ )\n",
        "fig.add_vline(x=0.2349, line_width=3, line_dash=\"solid\", line_color=\"red\", annotation_text=\"Post\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "P9-57zY3eq7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ê¸°ê°„(Pre/Post)ë³„ í† í”½ ë¹„ì¤‘ ê³„ì‚°\n",
        "topic_dist = df_clean.groupby(['Period', 'Topic_Name']).size().reset_index(name='Count')\n",
        "\n",
        "# 2. ë¹„ìœ¨(%)ë¡œ ë³€í™˜\n",
        "topic_dist['Percentage'] = topic_dist.groupby('Period')['Count'].transform(lambda x: x / x.sum())\n",
        "\n",
        "# 3. ì‹œê°í™” (100% Stacked Bar Chart)\n",
        "fig = px.bar(topic_dist,\n",
        "             x='Period',\n",
        "             y='Percentage',\n",
        "             color='Topic_Name',\n",
        "             title=\"<b>ì´ë²¤íŠ¸ ì „í›„ ë‰´ìŠ¤ í† í”½ êµ¬ì„± ë³€í™” (Composition Shift)</b>\",\n",
        "             text_auto='.1%',\n",
        "             category_orders={\"Period\": [\"Pre-Event\", \"Post-Event\"]} # ìˆœì„œ ê³ ì •\n",
        "            )\n",
        "\n",
        "fig.update_layout(\n",
        "    yaxis_title=\"ë¹„ì¤‘ (%)\",\n",
        "    template=\"plotly_white\",\n",
        "    legend=dict(orientation=\"h\", y=1.1, x=0, xanchor='left'), # ë²”ë¡€ ìœ„ë¡œ ì˜¬ë¦¬ê¸°\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "y2cYBSxy17qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tpOJ8bQg17oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì¸ê³¼ë¶„ì„"
      ],
      "metadata": {
        "id": "wj3M7v_iBse1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.stattools import grangercausalitytests, adfuller"
      ],
      "metadata": {
        "id": "dwKBxew8Chy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/gdrive/MyDrive/GMSW/project/result/'\n",
        "df = pd.read_csv(file_path+'news_topic_sentiment_result_ssk.csv')\n",
        "stock = pd.read_csv('/content/gdrive/MyDrive/GMSW/project/data/stock_price.csv')"
      ],
      "metadata": {
        "id": "rH-kmSE_DAqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# [News] ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ì‹œê°„ ì œê±° (YYYY-MM-DD)\n",
        "df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
        "\n",
        "# [News] í† í”½ë³„ ì¼ì¼ ë¹ˆë„ìˆ˜ ì§‘ê³„ (Pivot Table)\n",
        "# ì£¼ì˜: PPTì— ë‚˜ì˜¨ ì •í™•í•œ 'í† í”½ ì´ë¦„'ìœ¼ë¡œ í•„í„°ë§í•˜ê±°ë‚˜ ì´ë¦„ì„ ë§ì¶°ì£¼ì„¸ìš”.\n",
        "daily_topics = df.pivot_table(index='date', columns='Topic_Name', aggfunc='size', fill_value=0)"
      ],
      "metadata": {
        "id": "H8ZQVOJeDG1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Price] ë‚ ì§œ í˜•ì‹ ë³€í™˜\n",
        "stock['date'] = pd.to_datetime(stock['date'])\n",
        "stock.set_index('date', inplace=True)\n",
        "\n",
        "# [Price] ìˆ˜ìµë¥ (Return) ê³„ì‚° (ë¡œê·¸ ìˆ˜ìµë¥  ì¶”ì²œ - ì •ìƒì„± í™•ë³´ ìœ ë¦¬)\n",
        "# ë‘ ì¢…ëª©ì˜ í‰ê·  ìˆ˜ìµë¥ ì„ 'ë°˜ë„ì²´ ì„¹í„° ìˆ˜ìµë¥ 'ë¡œ ì •ì˜\n",
        "stock['samsung_ret'] = np.log(stock['samsung_price'] / stock['samsung_price'].shift(1))\n",
        "stock['skhynix_ret'] = np.log(stock['skhynix_price'] / stock['skhynix_price'].shift(1))\n",
        "stock['sector_return'] = (stock['samsung_ret'] + stock['skhynix_ret']) / 2\n",
        "\n",
        "# [Merge] ë°ì´í„° ë³‘í•© (Inner Joinìœ¼ë¡œ êµì§‘í•© ë‚ ì§œë§Œ ë¶„ì„)\n",
        "merged_df = pd.merge(daily_topics, stock[['sector_return']], left_index=True, right_index=True, how='inner')\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ ì œê±° (ìˆ˜ìµë¥  ê³„ì‚° ì²« ë‚ ì€ NaN ë°œìƒí•˜ë¯€ë¡œ ì œê±°)\n",
        "merged_df.dropna(inplace=True)\n",
        "\n",
        "print(f\"ë¶„ì„ ëŒ€ìƒ ë°ì´í„° ê¸°ê°„: {merged_df.index.min()} ~ {merged_df.index.max()}\")\n",
        "print(f\"ì´ ë¶„ì„ ì¼ìˆ˜: {len(merged_df)}ì¼\")"
      ],
      "metadata": {
        "id": "NYT-424ADWtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "foi_llaTEzpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 3. ê·¸ë ˆì¸ì € ì¸ê³¼ê²€ì • (Granger Causality Test)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def run_granger_test(data, x_col, y_col, max_lag=3):\n",
        "    \"\"\"\n",
        "    x_col: ì›ì¸ ë³€ìˆ˜ (ì˜ˆ: ë‰´ìŠ¤ ë¹ˆë„)\n",
        "    y_col: ê²°ê³¼ ë³€ìˆ˜ (ì˜ˆ: ì£¼ê°€ ìˆ˜ìµë¥ )\n",
        "    \"\"\"\n",
        "    print(f\"\\n[Test] '{x_col}'ê°€ '{y_col}'ì˜ ì›ì¸ì¸ê°€?\")\n",
        "\n",
        "    # ë°ì´í„°ì…‹ êµ¬ì„± [ì¢…ì†ë³€ìˆ˜(Y), ë…ë¦½ë³€ìˆ˜(X)] ìˆœì„œ ì£¼ì˜!\n",
        "    test_data = data[[y_col, x_col]]\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "    # verbose=Trueë¡œ í•˜ë©´ ìƒì„¸ ê²°ê³¼ ì¶œë ¥ë¨\n",
        "    gc_res = grangercausalitytests(test_data, maxlag=max_lag, verbose=False)\n",
        "\n",
        "    results = []\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        # F-testì˜ p-value ì¶”ì¶œ (ssr_ftest ì‚¬ìš©)\n",
        "        p_value = gc_res[lag][0]['ssr_ftest'][1]\n",
        "        f_score = gc_res[lag][0]['ssr_ftest'][0]\n",
        "        results.append({'Lag': lag, 'F-Score': f_score, 'P-Value': p_value})\n",
        "\n",
        "        significance = \"***\" if p_value < 0.01 else \"**\" if p_value < 0.05 else \"*\" if p_value < 0.1 else \"\"\n",
        "        print(f\"  Lag {lag}: P-value = {p_value:.4f} {significance}\")\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "laR8D4HdDgOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ë°ì´í„° ìˆ˜ì§‘: ëª¨ë“  í† í”½ì˜ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ DFë¡œ í†µí•©\n",
        "# ---------------------------------------------------------\n",
        "results_list = []\n",
        "\n",
        "# PPTì˜ í•µì‹¬ ë¹„êµêµ° (ì»¬ëŸ¬ ë§¤ì¹­ì„ ìœ„í•´ ìˆœì„œ ì¤‘ìš”)\n",
        "topics_to_test = ['í˜‘ë ¥ ê¸°ëŒ€ê°', 'ì‚°ì—…íŠ¸ë Œë“œ/ì‹¤ì /í€ë”ë©˜í„¸']\n",
        "existing_topics = [t for t in topics_to_test if t in merged_df.columns]\n",
        "\n",
        "if not existing_topics:\n",
        "    print(\"[!] ë°ì´í„°ì— í•´ë‹¹ í† í”½ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    for topic in existing_topics:\n",
        "        # ê·¸ë ˆì¸ì € ì¸ê³¼ê²€ì • ì‹¤í–‰ (ì´ì „ ë‹¨ê³„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ ì‚¬ìš©)\n",
        "        res_df = run_granger_test(merged_df, x_col=topic, y_col='sector_return', max_lag=5)\n",
        "\n",
        "        # 'Topic' ì»¬ëŸ¼ ì¶”ê°€ (ì–´ë–¤ ì£¼ì œì˜ ê²°ê³¼ì¸ì§€ êµ¬ë¶„)\n",
        "        res_df['Topic'] = topic\n",
        "        results_list.append(res_df)\n",
        "\n",
        "    # ìœ„ì•„ë˜ë¡œ ë°ì´í„° í•©ì¹˜ê¸°\n",
        "    final_result_df = pd.concat(results_list, ignore_index=True)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Plotly Expressë¡œ ì‹œê°í™”\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    # ìƒ‰ìƒ ì»¤ìŠ¤í…€: ê¸°ëŒ€ê°(ì„íŒ©íŠ¸) vs í€ë”ë©˜í„¸(ì´ì„±/ì¹¨ì°©)\n",
        "    custom_colors = {\n",
        "        'í˜‘ë ¥ ê¸°ëŒ€ê°': '#FF5733',          # ë¶‰ì€ ê³„ì—´ (ê°•ì¡°)\n",
        "        'ì‚°ì—…íŠ¸ë Œë“œ/ì‹¤ì /í€ë”ë©˜í„¸': '#3375FF' # íŒŒë€ ê³„ì—´ (ëŒ€ì¡°)\n",
        "    }\n",
        "\n",
        "    fig = px.line(\n",
        "        final_result_df,\n",
        "        x='Lag',\n",
        "        y='P-Value',\n",
        "        color='Topic',\n",
        "        markers=True,                 # ë°ì´í„° í¬ì¸íŠ¸ í‘œì‹œ\n",
        "        title='<b>Granger Causality Test</b>: News Topic â†’ Stock Return',\n",
        "        color_discrete_map=custom_colors, # ì§€ì •í•œ ìƒ‰ìƒ ì ìš©\n",
        "        template='plotly_white'       # ê¹”ë”í•œ í°ìƒ‰ ë°°ê²½ í…Œë§ˆ\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. ë””í…Œì¼ ê¾¸ë¯¸ê¸° (PPTìš© ìŠ¤íƒ€ì¼ë§)\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    # (1) ìœ ì˜ìˆ˜ì¤€ 0.05 ë¹¨ê°„ ì ì„  ì¶”ê°€\n",
        "    fig.add_hline(\n",
        "        y=0.05,\n",
        "        line_dash=\"dash\",\n",
        "        line_color=\"black\",\n",
        "        annotation_text=\"Significance Level (0.05)\",\n",
        "        annotation_position=\"bottom right\"\n",
        "    )\n",
        "\n",
        "    # (2) ë ˆì´ì•„ì›ƒ ë‹¤ë“¬ê¸°\n",
        "    fig.update_layout(\n",
        "        font=dict(family=\"Malgun Gothic\", size=14), # í°íŠ¸ ì„¤ì •\n",
        "        xaxis=dict(\n",
        "            title=\"Time Lag (Days)\",\n",
        "            tickmode='linear', # 1, 2, 3, 4, 5 ì •ìˆ˜ë§Œ í‘œì‹œ\n",
        "            dtick=1\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title=\"P-Value (Lower is More Causal)\",\n",
        "            range=[0, 0.5] # P-valueê°€ ë„ˆë¬´ íŠ€ë©´ ê·¸ë˜í”„ê°€ ë‚©ì‘í•´ì§€ë¯€ë¡œ yì¶• ë²”ìœ„ ê³ ì • (í•„ìš”ì‹œ ì¡°ì ˆ)\n",
        "        ),\n",
        "        legend=dict(\n",
        "            title=None,          # ë²”ë¡€ ì œëª© ì œê±°\n",
        "            orientation=\"h\",     # ë²”ë¡€ ê°€ë¡œë¡œ ë°°ì¹˜ (ê·¸ë˜í”„ í•˜ë‹¨)\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,              # ê·¸ë˜í”„ ìœ„ìª½ì— ë°°ì¹˜\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        ),\n",
        "        hovermode=\"x unified\"    # ë§ˆìš°ìŠ¤ ì˜¬ë ¸ì„ ë•Œ ë¹„êµí•˜ê¸° í¸í•˜ê²Œ\n",
        "    )\n",
        "\n",
        "    # ê·¸ë˜í”„ ì¶œë ¥\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "c0D0xdezzkXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOwKgdxhGhrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SiqQGa9GluF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzHi9SxyGlr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0wP5gJqGlqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}